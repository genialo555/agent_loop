# Sprint 2 Quick Start - QLoRA Fine-tuning avec Docker

Guide de d√©marrage rapide pour l'architecture Docker QLoRA du projet Agent Loop.

## üöÄ D√©marrage Rapide (5 minutes)

### 1. V√©rification des Pr√©requis
```bash
# V√©rifier GPU NVIDIA disponible
nvidia-smi

# V√©rifier Docker avec support GPU  
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu22.04 nvidia-smi

# V√©rifier Docker Compose v2
docker compose version
```

### 2. Test de l'Architecture
```bash
# Ex√©cuter suite de tests compl√®te
python scripts/test_docker_training.py

# R√©sultat attendu: All tests passed! ‚úÖ
```

### 3. Lancement Training D√©veloppement
```bash
# Environnement interactif complet  
make train-dev

# Acc√®s interfaces:
# üìö Jupyter: http://localhost:8888 (token: agent-dev-2024)
# üìä TensorBoard: http://localhost:6006
# üîç W&B Monitor: http://localhost:8080
```

### 4. Premier Fine-tuning QLoRA
```bash
# Training Gemma 2B avec donn√©es exemple
make train-gemma-2b

# Monitoring en temps r√©el
make train-status
make train-logs
```

---

## üìã Commandes Essentielles

### D√©veloppement
```bash
# Environnement d√©veloppement complet
make train-dev                    # Jupyter + TensorBoard + W&B

# Status et monitoring
make train-status                 # Status containers + GPU
make train-logs                   # Logs training temps r√©el  

# Gestion conteneurs
make train-stop                   # Arr√™t propre
make train-clean                  # Nettoyage complet (‚ö†Ô∏è supprime checkpoints)
```

### Training Production
```bash
# Mod√®les pr√©d√©finis optimis√©s
make train-gemma-2b              # Gemma 2B - RTX 4090 optimis√©
make train-gemma-9b              # Gemma 9B - m√©moire r√©duite

# Training custom
make train-custom MODEL=google/gemma-2-2b DATA=/path/to/data

# Monitoring complet (training + dev + monitoring)
make train-monitor
```

### Configuration Manuelle
```bash
# Docker Compose direct
docker compose -f docker-compose.training.yml --profile training up

# Training interactif  
docker compose -f docker-compose.training.yml --profile training-dev up -d

# Commande training custom
docker compose -f docker-compose.training.yml run --rm qlora-training \
  python training/qlora_finetune.py \
  --model-config gemma-2b \
  --data /app/datasets/processed \
  --output-dir /app/outputs/custom-run
```

---

## üèóÔ∏è Architecture Overview

### Dockerfile Multi-Stage
- **Stage training**: NVIDIA CUDA 12.6 + PyTorch GPU + QLoRA stack
- **Optimisations**: BuildKit cache, non-root user, memory management
- **Size**: ~8GB (optimis√© avec BuildKit cache)

### Docker Compose Training
- **qlora-training**: Service principal GPU avec ressources limit√©es
- **training-dev**: Jupyter Lab + TensorBoard pour d√©veloppement
- **training-monitor**: W&B Local + m√©triques GPU temps r√©el
- **gpu-monitor**: Surveillance GPU avec logs JSON
- **training-backup**: Backup automatique checkpoints (toutes les heures)

### Volumes Optimis√©s
```
‚îú‚îÄ‚îÄ training_models_cache/          # Cache HuggingFace (~2-5GB)
‚îú‚îÄ‚îÄ training_models_gguf/           # Mod√®les GGUF (~4-8GB)  
‚îú‚îÄ‚îÄ training_checkpoints/           # Checkpoints training (~1-3GB)
‚îú‚îÄ‚îÄ training_logs/                  # Logs structur√©s + m√©triques GPU
‚îî‚îÄ‚îÄ training_backup/                # Backup automatique s√©curis√©
```

---

## ‚öôÔ∏è Configuration QLoRA

### RTX 4090 Optimis√©e (24GB VRAM)
```python
# training/qlora_config.py - GEMMA_2B_CONFIG
per_device_train_batch_size: 4      # Batch optimal RTX 4090
gradient_accumulation_steps: 4      # Effective batch = 16
bf16: True                          # Meilleur que fp16 sur Ampere
gradient_checkpointing: True        # Optimisation m√©moire
optim: "adamw_8bit"                # Optimizer 8-bit

# QLoRA 4-bit + Double Quantization
load_in_4bit: True
bnb_4bit_use_double_quant: True    # √âconomie m√©moire suppl√©mentaire  
bnb_4bit_quant_type: "nf4"         # Normal Float 4-bit optimal
lora_r: 32                         # Rank augment√© pour performance
target_modules: "all-linear"       # Cible tous les layers lin√©aires
```

### Configurations Pr√©d√©finies
- **Gemma 2B**: Optimis√© RTX 4090 (batch_size=4, 1000 steps)
- **Gemma 9B**: M√©moire r√©duite (batch_size=2, 800 steps)  
- **Custom**: Support mod√®les HuggingFace compatibles

---

## üìä Monitoring et Observabilit√©

### Interfaces Web
- **Jupyter Lab**: http://localhost:8888 (token: agent-dev-2024)
- **TensorBoard**: http://localhost:6006 (m√©triques training)
- **W&B Local**: http://localhost:8080 (experiments tracking)

### Monitoring GPU en Temps R√©el
```bash
# M√©triques GPU structur√©es (JSON)
tail -f logs/gpu_metrics.jsonl

# nvidia-smi classique
watch -n 1 nvidia-smi

# Status containers
docker stats agent-qlora-training
```

### Logs Structur√©s
```bash
# Logs training principal
docker logs -f agent-qlora-training

# Logs GPU monitor  
docker logs -f agent-gpu-monitor

# Logs backup automatique
docker logs agent-training-backup
```

---

## üîß Troubleshooting Rapide

### GPU Non D√©tect√©
```bash
# V√©rifier drivers NVIDIA
nvidia-smi

# V√©rifier Docker GPU support
docker run --rm --gpus all nvidia/cuda:12.6.0-base-ubuntu22.04 nvidia-smi

# Installer nvidia-container-toolkit si n√©cessaire
sudo apt install nvidia-container-toolkit
sudo systemctl restart docker
```

### Out of Memory (OOM)
1. **R√©duire batch size**: Modifier `per_device_train_batch_size` dans `qlora_config.py`
2. **Utiliser Gemma 2B**: Plus petit que Gemma 9B
3. **V√©rifier 4-bit quantization**: `load_in_4bit: True`
4. **Gradient checkpointing**: D√©j√† activ√© par d√©faut

### Build Docker Lent
```bash
# Builds subs√©quents utilisent le cache BuildKit
export DOCKER_BUILDKIT=1

# Build avec cache inline
docker build --cache-from agent-loop:latest .

# Nettoyage si cache corrompu
docker builder prune
```

### Containers Ne D√©marrent Pas
```bash
# V√©rifier configuration
docker compose -f docker-compose.training.yml config

# Logs d√©taill√©s
docker compose -f docker-compose.training.yml logs

# Reset complet
make train-clean
docker system prune -f
```

---

## üìö Documentation Compl√®te

- **Architecture**: `README_DOCKER.md` (section QLoRA Fine-tuning)
- **Configuration**: `training/qlora_config.py` (param√®tres d√©taill√©s)  
- **Pipeline**: `training/qlora_finetune.py` (code training)
- **Tests**: `scripts/test_docker_training.py` (validation architecture)

---

## üéØ Prochaines √âtapes

1. **Tester architecture**: `python scripts/test_docker_training.py`
2. **Pr√©parer donn√©es**: Placer datasets dans `datasets/processed/`
3. **T√©l√©charger mod√®le base**: Voir `models/README.md`
4. **Lancer premier training**: `make train-gemma-2b`
5. **Monitoring**: Utiliser interfaces web pour suivre progression

### Training Custom
```bash
# Exemple avec vos donn√©es
make train-custom \
  MODEL=google/gemma-2-2b \
  DATA=/path/to/your/training/data
```

### Int√©gration CI/CD
L'architecture est pr√™te pour int√©gration dans pipeline MLOps avec tests automatis√©s et d√©ployment containeris√©.

---

**üèÅ Sprint 2 Ready!** Architecture Docker QLoRA op√©rationnelle avec GPU RTX 4090, monitoring avanc√© et bonnes pratiques ML/DevOps.