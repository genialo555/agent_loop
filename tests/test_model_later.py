#!/usr/bin/env python3
"""
Script de test pour √©valuer le mod√®le Gemma-3N fine-tun√©
√Ä utiliser APR√àS l'entra√Ænement (au moins 5000 steps avec loss < 1.2)
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from datetime import datetime

# Test basique de navigation OS
test_prompts = [
    # Test 1: Commande simple
    "List all Python files in the current directory and subdirectories",
    
    # Test 2: Debug basique  
    "Find and display the last 10 error messages in the system logs",
    
    # Test 3: Analyse de performance
    "Check which process is using the most CPU and memory right now",
    
    # Test 4: Gestion de fichiers
    "Find all files larger than 100MB in /home and sort them by size",
    
    # Test 5: T√¢che complexe
    "Create a backup script that archives all .log files older than 7 days to /backup/ with today's date"
]

def test_model(model_path):
    """Test le mod√®le fine-tun√©"""
    print(f"üîç Loading model from: {model_path}")
    
    # Charger le mod√®le et tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )
    
    print("‚úÖ Model loaded!\n")
    
    # Tester chaque prompt
    for i, prompt in enumerate(test_prompts, 1):
        print(f"{'='*60}")
        print(f"Test {i}: {prompt}")
        print(f"{'='*60}")
        
        # Format du prompt syst√®me
        full_prompt = f"""You are an AI assistant that helps with Linux system administration tasks.
Provide the exact commands needed to accomplish the following task:

Task: {prompt}

Commands:"""
        
        # Tokenizer et g√©n√©rer
        inputs = tokenizer(full_prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=256,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        # Extraire seulement la partie g√©n√©r√©e
        response = response.split("Commands:")[-1].strip()
        
        print(f"Response:\n{response}\n")
        
        # √âvaluation basique
        has_command = any(cmd in response.lower() for cmd in ['find', 'ls', 'grep', 'ps', 'du', 'tar'])
        print(f"‚úì Contains command: {'Yes' if has_command else 'No'}")
        print()

def check_training_status():
    """V√©rifie si le mod√®le est pr√™t pour les tests"""
    try:
        import json
        with open("results/gemma-3n-safe-1epoch/trainer_state.json", "r") as f:
            state = json.load(f)
        
        last_loss = state['log_history'][-1].get('loss', 999)
        total_steps = state['global_step']
        
        print(f"üìä Training Status:")
        print(f"   Steps: {total_steps}")
        print(f"   Last Loss: {last_loss:.4f}")
        
        if total_steps < 5000:
            print(f"‚ö†Ô∏è  Model needs more training! (Current: {total_steps}, Recommended: >5000)")
            return False
        elif last_loss > 1.5:
            print(f"‚ö†Ô∏è  Loss still high! (Current: {last_loss:.4f}, Recommended: <1.2)")
            return False
        else:
            print("‚úÖ Model ready for testing!")
            return True
            
    except:
        print("‚ùå No training state found. Model not ready.")
        return False

if __name__ == "__main__":
    print("üöÄ Gemma-3N Agent Testing Script\n")
    
    # V√©rifier si le mod√®le est pr√™t
    if not check_training_status():
        print("\n‚è≥ Come back later when the model has trained more!")
        print("üí° Run this command to check progress:")
        print("   tail -f results/gemma-3n-safe-1epoch/trainer_state.json | grep loss")
    else:
        # Chemin du mod√®le sauvegard√©
        model_path = "./results/gemma-3n-safe-1epoch"
        test_model(model_path)
        
    print(f"\nüïê Generated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")