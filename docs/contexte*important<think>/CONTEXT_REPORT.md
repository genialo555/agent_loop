# üìä Rapport de Contexte Complet - Gemma-3N-Agent-Loop

## üéØ Vue d'Ensemble du Projet

**Gemma-3N-Agent-Loop** est un syst√®me MLOps sophistiqu√© impl√©mentant une boucle d'apprentissage continu pour des agents IA bas√©s sur les mod√®les Gemma de Google. Le projet suit une architecture de production avec des pratiques DevOps/MLOps avanc√©es.

### Cycle de Vie Principal
```
Pre-train ‚Üí Deploy ‚Üí Log ‚Üí Fine-tune ‚Üí Redeploy
```

## üèóÔ∏è Architecture Technique

### Architecture Hexagonale (Score: 9/10)
- **Core Domain**: Logique m√©tier pure dans `/core/`
- **Ports**: Interfaces dans `/inference/services/`
- **Adapters**: FastAPI routers et services externes
- **Infrastructure**: Docker, Terraform, Ansible

### Stack Technique
- **Backend**: FastAPI avec patterns async/await complets
- **ML Framework**: PyTorch + HuggingFace Transformers
- **Inference**: Ollama pour le d√©ploiement optimis√© (GGUF)
- **Monitoring**: Prometheus + Grafana + Weights&Biases
- **Orchestration**: Prefect pour les pipelines ML
- **S√©curit√©**: Framework multi-couches contre les prompt injections

## üìà √âtat Actuel (Juillet 2025)

### ‚úÖ Compl√©t√© (Sprint 1)
1. **Infrastructure MLOps compl√®te**
   - Pipeline CI/CD (8 √©tapes)
   - D√©ploiement Blue-Green
   - Monitoring complet
   - Model Registry

2. **API FastAPI Production**
   - Endpoints health/ready/metrics
   - WebSocket pour streaming
   - Middleware de s√©curit√©
   - Endpoint `/run-agent` basique

3. **Int√©gration Ollama**
   - Support GPU/CPU
   - G√©n√©ration de texte
   - Gestion des mod√®les

### ‚ö†Ô∏è Probl√®mes Actuels
1. **M√©moire GPU insuffisante**
   - RTX 3090 (24GB) OOM avec gemma-3n-e4b
   - N√©cessite optimisation batch size

2. **Endpoint `/run-agent` basique**
   - Utilise regex au lieu d'Ollama
   - Pas de JWT authentication
   - Pas de vrai raisonnement LLM

3. **Tests √† 39.4% de couverture**
   - Objectif: 80%
   - Modules training et CLI √† 0%

## üíæ Configuration et Stockage

### Mod√®les
- **Cache**: `/media/jerem/jeux&travail/ml_models/` (SSD externe)
- **Checkpoints**: `/home/jerem/agent_loop/model_checkpoints/`
- **Mod√®les t√©l√©charg√©s**: 

  - google/gemma-3n-e4b (sharded)

### Datasets
- **Localisation**: `/media/jerem/jeux&travail/datasets/`
- **Types**: agent_instruct, toolbench, browsergym, webArena, etc.
- **S√©curit√©**: Validation contre prompt injections

## üõ°Ô∏è S√©curit√©

### Framework de S√©curit√© Multi-Niveaux
1. **D√©tection avanc√©e** (50+ patterns)
   - Prompt injection
   - XSS/code injection
   - Anomalies statistiques

2. **Sandboxing**
   - Isolation process-level
   - Conteneurs Docker
   - Limites de ressources

3. **Monitoring temps r√©el**
   - Alertes email/webhook
   - Threat intelligence
   - Anomaly detection

### Politiques: Minimal ‚Üí Standard ‚Üí Strict ‚Üí Maximum

## üöÄ Sprint 2 - Objectifs Imm√©diats

1. **Int√©gration Ollama dans `/run-agent`**
   ```python
   # Remplacer le regex par:
   result = await ollama_service.generate(instruction)
   ```

2. **Ajout JWT Authentication**
   ```python
   @router.post("/run-agent", dependencies=[Depends(verify_jwt)])
   ```

3. **R√©solution OOM GPU**
   - R√©duire batch_size de 16 √† 8
   - Activer Flash Attention
   - Impl√©menter torch.compile

4. **Tests prioritaires**
   - QLoRA configuration
   - CLI commands
   - Training pipeline

## üìã Commandes Utiles

### D√©veloppement
```bash
make install          # Installation
make lint             # V√©rification code
make test             # Tests
make format           # Formatage
```

### Training
```bash
make train-docker     # Training dans Docker
make train-gemma-2b   # Train Gemma 2B
make gpu-monitor      # Monitor GPU
```

### D√©ploiement
```bash
make deploy-staging   # Staging
make deploy-prod      # Production
make rollback         # Rollback
```

### Commande Sp√©ciale
```
@LES_GARS <request>   # Active tous les agents sp√©cialis√©s
```

## üéØ M√©triques Cibles

### Performance
- Response time: P95 < 5s
- GPU utilization: < 80%
- Memory efficiency: 60% reduction

### Qualit√©
- Test coverage: > 80%
- Type coverage: 100%
- Zero linting errors

### Business
- Zero co√ªt op√©rationnel (inf√©rence locale)
- 100% confidentialit√© des donn√©es
- Capacit√© offline

## üîÆ Vision Long Terme

1. **Architecture Hybride**
   - Training: HuggingFace/PyTorch
   - Inference: Ollama/GGUF

2. **Features Avanc√©es**
   - Multi-model support
   - Federated learning
   - Edge deployment
   - AutoML pipeline

3. **Production Features**
   - Kubernetes migration
   - Multi-cloud support
   - A/B testing framework
   - Model ensemble

## üìù Points d'Attention

1. **GPU Memory**: Surveiller l'utilisation, autre process utilise 5.8GB
2. **Python Version**: Standardiser sur 3.11.x (incoh√©rence actuelle)
3. **Security Middleware**: Activer les middlewares comment√©s
4. **Monitoring**: Lancer Prometheus/Grafana pour visibilit√©

---

*Rapport g√©n√©r√© le 30 Juillet 2025 - Bas√© sur l'analyse compl√®te par les agents sp√©cialis√©s*