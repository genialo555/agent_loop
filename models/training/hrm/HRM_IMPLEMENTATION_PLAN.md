# üß† Plan d'Impl√©mentation HRM pour Gemma-3N

## üìã R√©sum√© Ex√©cutif

**Objectif**: Adapter l'architecture Hierarchical Reasoning Model (HRM) √† Gemma-3N (4.5B params) pour am√©liorer les capacit√©s de raisonnement hi√©rarchique et la r√©solution de probl√®mes complexes.

**Approche**: Impl√©menter les deux modules r√©currents (H et L) de HRM comme des adaptateurs LoRA sur Gemma-3N, en utilisant Unsloth pour l'optimisation et en conservant l'architecture de base.

## üèóÔ∏è Architecture HRM-Gemma-3N

### 1. Structure Modulaire

```python
HRM-Gemma-3N:
‚îú‚îÄ‚îÄ Module Bas-Niveau (L) - Calculs rapides et d√©taill√©s
‚îÇ   ‚îú‚îÄ‚îÄ LoRA Adapters sur attention layers (rank=32)
‚îÇ   ‚îú‚îÄ‚îÄ Hidden state: 2048 dims
‚îÇ   ‚îî‚îÄ‚îÄ Update frequency: Every timestep
‚îÇ
‚îú‚îÄ‚îÄ Module Haut-Niveau (H) - Planification abstraite
‚îÇ   ‚îú‚îÄ‚îÄ LoRA Adapters sur FFN layers (rank=64)
‚îÇ   ‚îú‚îÄ‚îÄ Hidden state: 4096 dims
‚îÇ   ‚îî‚îÄ‚îÄ Update frequency: Every T timesteps
‚îÇ
‚îî‚îÄ‚îÄ Gemma-3N Base (frozen ou QLoRA)
    ‚îú‚îÄ‚îÄ 36 transformer layers
    ‚îú‚îÄ‚îÄ Hidden dim: 3584
    ‚îî‚îÄ‚îÄ Attention heads: 32
```

### 2. M√©canisme de Convergence Hi√©rarchique

```python
# Pseudo-code du forward pass
def hrm_forward(x, N=4, T=8):
    # N = nombre de cycles haut-niveau
    # T = timesteps par cycle bas-niveau
    
    x_emb = gemma_embed(x)
    z_L = init_low_state()
    z_H = init_high_state()
    
    for cycle in range(N):
        # Phase bas-niveau (convergence locale)
        for t in range(T):
            z_L = low_level_update(z_L, z_H, x_emb)
        
        # Mise √† jour haut-niveau
        z_H = high_level_update(z_H, z_L)
        
        # Reset bas-niveau pour nouveau cycle
        z_L = reset_with_context(z_H)
    
    return decode_output(z_H)
```

## üìÅ Structure des Fichiers

```
models/training/hrm/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ hrm_config.py              # Configuration HRM
‚îú‚îÄ‚îÄ hrm_model.py               # Architecture HRM-Gemma
‚îú‚îÄ‚îÄ hrm_modules.py             # Modules H et L
‚îú‚îÄ‚îÄ hrm_trainer.py             # Training avec Unsloth
‚îú‚îÄ‚îÄ approximate_gradient.py     # 1-step gradient
‚îú‚îÄ‚îÄ deep_supervision.py        # Deep supervision loop
‚îú‚îÄ‚îÄ adaptive_compute.py        # ACT avec Q-learning
‚îî‚îÄ‚îÄ hierarchical_convergence.py # M√©canisme de convergence
```

## üîß Impl√©mentation D√©taill√©e

### Phase 1: Architecture de Base (Priorit√©: HIGH)

1. **Cr√©er les modules H et L comme LoRA adapters**
   ```python
   # hrm_modules.py
   class LowLevelModule(nn.Module):
       def __init__(self, gemma_config):
           self.lora_attn = LoRALayer(rank=32, targets=['q', 'v'])
           self.hidden_dim = 2048
           self.state_projector = nn.Linear(3584, 2048)
   
   class HighLevelModule(nn.Module):
       def __init__(self, gemma_config):
           self.lora_ffn = LoRALayer(rank=64, targets=['gate', 'up'])
           self.hidden_dim = 4096
           self.state_projector = nn.Linear(3584, 4096)
   ```

2. **Impl√©menter la convergence hi√©rarchique**
   - Module L converge rapidement (8-16 steps)
   - Module H update seulement apr√®s convergence de L
   - Reset de L avec nouveau contexte de H

### Phase 2: Gradient Approximation (Priorit√©: HIGH)

1. **Impl√©menter 1-step gradient**
   ```python
   # approximate_gradient.py
   def compute_hrm_gradient(model, loss):
       # D√©tacher tous sauf derniers √©tats
       with torch.no_grad():
           # Forward pass complet sauf derni√®re √©tape
           states = forward_no_grad(model, N*T-1)
       
       # Gradient seulement sur derni√®re √©tape
       final_z_L = model.L_step(states['z_L'], states['z_H'])
       final_z_H = model.H_step(states['z_H'], final_z_L)
       output = model.decode(final_z_H)
       
       # Backprop O(1) m√©moire
       loss = criterion(output, target)
       loss.backward()
   ```

2. **Avantages pour RTX 3090**
   - M√©moire O(1) au lieu de O(T)
   - Permet batch_size plus large
   - Compatible avec gradient checkpointing

### Phase 3: Deep Supervision (Priorit√©: HIGH)

1. **Training loop avec supervision multiple**
   ```python
   # deep_supervision.py
   def train_with_deep_supervision(model, data, M=3):
       z = initialize_states()
       total_loss = 0
       
       for segment in range(M):
           # Forward pass segment
           z, output = model.hrm_forward(x, z)
           loss = criterion(output, y)
           
           # Mise √† jour imm√©diate
           optimizer.zero_grad()
           loss.backward()
           optimizer.step()
           
           # D√©tacher pour prochain segment
           z = z.detach()
           total_loss += loss.item()
       
       return total_loss / M
   ```

### Phase 4: Adaptive Computation Time (Priorit√©: MEDIUM)

1. **Q-Learning pour d√©cision halt/continue**
   ```python
   # adaptive_compute.py
   class QHead(nn.Module):
       def __init__(self, hidden_dim):
           self.q_net = nn.Linear(hidden_dim, 2)  # [halt, continue]
       
       def should_halt(self, z_H, m_current, m_max):
           q_values = torch.sigmoid(self.q_net(z_H))
           q_halt, q_continue = q_values[0], q_values[1]
           
           if m_current >= m_max:
               return True
           return q_halt > q_continue
   ```

## üöÄ Training Strategy avec Unsloth

### Configuration Optimale

```python
# hrm_trainer.py
from unsloth import FastLanguageModel

# Charger Gemma-3N avec Unsloth
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="/media/jerem/jeux&travail/ml_models/hub/models--google--gemma-3n-e4b",
    max_seq_length=2048,  # R√©duit pour HRM
    dtype=torch.float16,
    load_in_4bit=True,
)

# Ajouter modules HRM
model = add_hrm_modules(model, 
    lora_r_low=32,
    lora_r_high=64,
    lora_alpha=16,
    lora_dropout=0.05,
)

# Configuration training
training_args = TrainingArguments(
    per_device_train_batch_size=1,  # RTX 3090 limite
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    learning_rate=2e-4,
    warmup_steps=100,
    logging_steps=10,
    save_steps=500,
    output_dir="./results/gemma-3n-hrm",
    # HRM sp√©cifique
    hrm_cycles_N=4,
    hrm_timesteps_T=8,
    deep_supervision_M=3,
)
```

### Datasets HRM pour Gemma-3N

1. **Datasets disponibles** (d√©j√† dans HF cache):
   - GSM8K: Raisonnement math√©matique √©tape par √©tape
   - CodeAlpaca-20k: G√©n√©ration de code structur√©
   - Python Code Instructions 18k: Solutions hi√©rarchiques
   - SQL Create Context: D√©composition de requ√™tes

2. **Format unifi√© HRM**:
   ```json
   {
     "instruction": "R√©soudre √©tape par √©tape: ...",
     "high_level_plan": ["√âtape 1: Analyser", "√âtape 2: D√©composer", ...],
     "low_level_steps": [
       ["sous-√©tape 1.1", "sous-√©tape 1.2"],
       ["sous-√©tape 2.1", "sous-√©tape 2.2"]
     ],
     "final_output": "Solution compl√®te"
   }
   ```

## üìä M√©triques et √âvaluation

### Benchmarks Cibles

1. **Raisonnement Structur√©**
   - GSM8K accuracy: >70% (baseline: ~45%)
   - Code generation: >85% syntaxe correcte
   - Tool use accuracy: >95%

2. **M√©triques HRM Sp√©cifiques**
   - Convergence L-module: <10 steps
   - Utilisation cycles H: 2-6 selon complexit√©
   - Temps d'inf√©rence: <2s par requ√™te

### Monitoring

```python
# M√©triques √† tracker
metrics = {
    'hrm/l_convergence_steps': [],  # Steps avant convergence L
    'hrm/h_cycles_used': [],         # Nombre de cycles H utilis√©s
    'hrm/forward_residuals': [],     # Activit√© computationnelle
    'hrm/q_values': [],              # D√©cisions halt/continue
    'hrm/memory_usage': [],          # GPU memory avec O(1)
}
```

## üõ†Ô∏è Optimisations RTX 3090

1. **M√©moire**
   - Gradient approximation O(1): √©conomise ~10GB
   - LoRA ranks adapt√©s: L=32, H=64
   - Flash Attention 2 activ√©

2. **Performance**
   - Batch size effectif: 8 (avec accumulation)
   - Mixed precision (fp16)
   - Unsloth kernel optimizations

## üìÖ Planning d'Impl√©mentation

### Sprint 1 (1 semaine)
- [x] Analyse architecture HRM
- [ ] Cr√©er structure fichiers
- [ ] Impl√©menter modules H et L basiques
- [ ] Test forward pass simple

### Sprint 2 (1 semaine)
- [ ] Convergence hi√©rarchique compl√®te
- [ ] 1-step gradient approximation
- [ ] Integration avec Unsloth
- [ ] Tests unitaires

### Sprint 3 (1 semaine)
- [ ] Deep supervision training
- [ ] Adaptive Computation Time
- [ ] Benchmarks initiaux
- [ ] Optimisations m√©moire

### Sprint 4 (1 semaine)
- [ ] Fine-tuning sur datasets HRM
- [ ] √âvaluation compl√®te
- [ ] Documentation
- [ ] Int√©gration API

## üéØ R√©sultats Attendus

1. **Performance**
   - Am√©lioration 25-40% sur raisonnement structur√©
   - R√©duction latence avec ACT (skip cycles inutiles)
   - Meilleure g√©n√©ralisation avec peu d'exemples

2. **Efficacit√©**
   - Training 3x plus rapide qu'avec BPTT
   - Inf√©rence adaptative selon complexit√©
   - Utilisation m√©moire constante

3. **Applications**
   - Agent Linux avec planification hi√©rarchique
   - D√©composition automatique de t√¢ches
   - Raisonnement multi-√©tapes robuste

## üìù Notes d'Impl√©mentation

1. **Compatibilit√© Gemma-3N**
   - Utiliser Unsloth exclusivement
   - Adapter dimensions hidden states
   - Pr√©server architecture AltUp

2. **Int√©gration Projet**
   - Suivre structure `models/training/hrm/`
   - R√©utiliser `core/settings.py`
   - Logger avec structlog

3. **Tests Critiques**
   - Convergence sur t√¢ches simples
   - Stabilit√© gradient approximation
   - Performance vs baseline

---

**Prochaine √©tape**: Commencer par cr√©er `hrm_modules.py` avec les classes LowLevelModule et HighLevelModule.