
  
  # ğŸ¤– Agent Loop - Autonomous LLM Training & Inference Platform

  > **Production-ready MLOps platform** for training, deploying, and continuously improving autonomous agents powered by **Gemma 3N**.

  [![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
  [![FastAPI](https://img.shields.io/badge/FastAPI-0.104+-green.svg)](https://fastapi.tiangolo.com/)
  [![Docker](https://img.shields.io/badge/docker-ready-blue.svg)](https://www.docker.com/)
  [![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
</div>

---

## ğŸ“‘ Documentation Index

> **Navigation Guide:**
> - ğŸ§‘â€ğŸ’» **Human Users**: Start with Overview and Quick Start sections
> - ğŸ¤– **AI Agents**: Start with CLAUDE.md for primary context
> - ğŸ“ **Location Types**: `README.md` (this file), `CLAUDE.md` (root), `External` (separate file), `Directory` (folder)

### ğŸ§‘â€ğŸ’» For @les_gars*& Human Users
| Section | Description | Location |
|---------|-------------|----------|
| **Getting Started** | | |
| [ğŸ¯ Overview](#-overview) | High-level project introduction | README.md |
| [ğŸš€ Quick Start](#-quick-start) | Getting started guide | README.md |
| [ğŸ“ Project Structure](#-project-structure) | Directory organization | README.md |
| **Development** | | |
| [ğŸ”„ Training Workflow](#-training-workflow) | How to train models | README.md |
| [Training Commands](docs/TRAINING_COMMANDS_humain.md) | Human-readable training guide | External |
| [ğŸ“Š Monitoring](#-monitoring--observability) | Dashboards and metrics | README.md |
| [ğŸ§ª Testing](#-testing) | Running tests | README.md |
| **Architecture & Design** | | |
| [Architecture Guide](docs/ARCHITECTURE/ARCHITECTURE.md) | System design overview | External |
| [MLOps Architecture](docs/ARCHITECTURE/MLOPS_ARCHITECTURE.md) | ML pipeline design | External |
| [Deployment Procedures](docs/ARCHITECTURE/DEPLOYMENT_PROCEDURES.md) | Deploy guide | External |
| **Security & Ops** | | |
| [Security Analysis](docs/SECURITY/SECURITY_ANALYSIS_REPORT.md) | Security audit | External |
| [Observability Setup](docs/observabilty/OBSERVABILITY_IMPLEMENTATION_GUIDE.md) | Monitoring guide | External |
| **Research & Development** | | |
| [LLM Research](docs/R&D/LLM_RESEARCH_AND_INTEGRATION.md) | LLM integration | External |
| [Gemma 3N Reference](docs/R&D/GEMMA3N_TECHNICAL_REFERENCE.md) | Model details | External |
| [Unsloth Examples](docs/R&D/UNSLOTH_TRAINING_EXAMPLES.md) | Training examples | External |

### ğŸ¤– For Machine Users (AI Agents)
| Section | Description | Location |
|---------|-------------|----------|
| **Primary Context** | | |
| [CLAUDE.md](CLAUDE.md) | âš¡ **MAIN INSTRUCTIONS** - Start here | Root |
| [Project Structure](docs/contexte*important<THINK>/PROJECT_STRUCTURE.md) | Directory architecture | External |
| [Training Context](docs/contexte*important<THINK>/CONTEXT_TRAINING.md) | Training specifics | External |
| **Agent Configuration** | | |
| [Agent Orchestrator](.claude/agents/agent-orchestrator.md) | Main coordinator | External |
| [Specialized Agents](.claude/agents/) | All agent configs | Directory |
| [Coordination Matrix](.claude/agents/AGENT_COORDINATION_MATRIX.md) | Agent interactions | External |
| [Inter-Agent Protocol](.claude/agents/INTER_AGENT_PROTOCOL.md) | Communication rules | External |
| **Rules & Constraints** | | |
| [Critical Rules](CLAUDE.md#critical-rules---no-errors-allowed-memorize) | âš ï¸ <think>Must-follow <think> | CLAUDE.md |
| [Import Patterns](CLAUDE.md#import-pattern) | Code conventions | CLAUDE.md |
| [XML Rules](docs/rules/) | Structured rule files | Directory |
| [File Monitoring](.claude/rules/file_monitoring_strict.md) | File system rules | External |
| **Technical References** | | |
| [Architecture Diagrams](#-architecture-overview) | Mermaid diagrams | README.md |
| [API Endpoints](#health-endpoints) | Technical endpoints | README.md |
| [System Prompts](prompts/system_prompt.txt) | LLM instructions | External |
| [Context Reports](docs/contexte*important<THINK>/) | Analysis reports | Directory |

### ğŸ”§ Technical References
| Topic | Audience | Location |
|-------|----------|----------|
| [Model Storage](#critical-model-storage-memorize) | Both | CLAUDE.md |
| [HuggingFace Cache](#critical-huggingface-cache-memorize) | Both | CLAUDE.md |
| [Performance Targets](#-performance-targets) | Both | README.md |
| [Hardware Config](#hardware-configuration) | Both | CLAUDE.md |
| [Common Workflows](#common-workflows) | Both | CLAUDE.md |
| [Troubleshooting](#troubleshooting-tips) | Both | CLAUDE.md |

### ğŸ“‚ Key Directories by User Type
```
ğŸ§‘â€ğŸ’» HUMAN-FOCUSED:
â”œâ”€â”€ docs/TRAINING_COMMANDS_humain.md    # Human training guide
â”œâ”€â”€ docs/ARCHITECTURE/                   # System design
â”œâ”€â”€ docs/SECURITY/                       # Security docs
â”œâ”€â”€ docs/R&D/                           # Research papers
â”œâ”€â”€ docs/observabilty/                  # Monitoring guides
â”œâ”€â”€ docs/scrummaster/                   # Sprint planning
â”œâ”€â”€ README.md                           # This file
â””â”€â”€ tests/                              # Test suites

ğŸ¤– MACHINE-FOCUSED:
â”œâ”€â”€ CLAUDE.md                           # AI agent instructions âš¡
â”œâ”€â”€ .claude/                            # Agent configurations
â”‚   â”œâ”€â”€ agents/                         # Specialized agents
â”‚   â”œâ”€â”€ rules/                          # Agent rules
â”‚   â””â”€â”€ commands/                       # Agent commands
â”œâ”€â”€ docs/contexte*important<THINK>/     # Machine context âš ï¸
â”œâ”€â”€ docs/rules/                         # XML rule files
â”œâ”€â”€ docs/claude'srules/                 # Claude-specific rules
â”œâ”€â”€ prompts/                            # System prompts
â””â”€â”€ agent/prompts/                      # Agent prompts
```

### ğŸ” Quick Reference for AI Agents

**âš¡ MUST READ FIRST:**
1. `CLAUDE.md` - Complete project context and rules
2. `docs/contexte*important<THINK>/PROJECT_STRUCTURE.md` - Directory layout
3. `.claude/agents/agent-orchestrator.md` - Coordination instructions

**âš ï¸ CRITICAL PATHS TO MEMORIZE:**
```bash
# Model Storage (ALL models go here)
/media/jerem/641C8D6C1C8D3A56/MLLMODELS/

# HuggingFace Cache (58GB+ - NEVER DELETE)
/media/jerem/641C8D6C1C8D3A56/hf_cache/

# Datasets
/media/jerem/jeux&travail/datasets/
```

---

## ğŸ¯ Overview

**Agent Loop** is a comprehensive MLOps platform that implements a closed training loop for autonomous agents:

1. **ğŸ“ Train** â†’ Fine-tune Gemma 3N models with QLoRA on curated datasets
2. **ğŸš€ Deploy** â†’ Serve models via FastAPI with async architecture  
3. **ğŸ“Š Monitor** â†’ Track performance with Prometheus & Grafana
4. **ğŸ”„ Iterate** â†’ Collect interaction logs and retrain continuously

### Key Features

- **ğŸ§  Advanced Model Architecture**: Gemma 3N with XNet heads, LoRA adapters, and GroupThink decoding
- **âš¡ Production-Ready API**: Async FastAPI with health checks, metrics, and security
- **ğŸ—ï¸ Infrastructure as Code**: Complete Terraform + Ansible deployment
- **ğŸ“ˆ Full Observability**: Structured logging, Prometheus metrics, Grafana dashboards
- **ğŸ”’ Security First**: JWT authentication, input validation, sandboxed execution
- **ğŸ³ Container Native**: Multi-stage Docker builds with optimization

---

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TB
    subgraph "ğŸš€ GEMMA-3N-AGENT-LOOP PLATFORM"
        subgraph "Core Services"
            API["ğŸŒ FastAPI Service<br/>- Async Endpoints<br/>- JWT Auth (RS256)<br/>- Health Checks<br/>- Rate Limiting"]
            OLLAMA["ğŸ¦ Ollama Engine<br/>- Gemma 3N (4.5B)<br/>- GGUF Format<br/>- Memory Optimized<br/>- Hot Reload"]
            AGENT["ğŸ¤– Agent Core<br/>- Tool Orchestration<br/>- Browser Plugin<br/>- Safety Filters<br/>- Sandboxed Exec"]
        end

        subgraph "ML Pipeline"
            TRAIN["ğŸ“ Training Pipeline<br/>- QLoRA (4-bit)<br/>- Unsloth Framework<br/>- XNet Heads<br/>- Gradient Checkpoint"]
            EVAL["âœ… Evaluation<br/>- ToolBench (95%+)<br/>- JSON Validation<br/>- A/B Testing<br/>- Auto Promotion"]
            DATA["ğŸ“š Datasets<br/>- Agent Instruct (1.1M)<br/>- HRM Datasets<br/>- ToolBench<br/>- WebArena"]
        end

        subgraph "Storage Layer"
            MODELS["ğŸ§  Model Storage<br/>/media/.../MLLMODELS/<br/>- Base Models<br/>- LoRA Adapters<br/>- GGUF Exports"]
            HF["ğŸ’¾ HF Cache (58GB)<br/>/media/.../hf_cache/<br/>- Cached Datasets<br/>- Pretrained Models"]
            LOGS["ğŸ“Š Logs & Metrics<br/>- Training Logs<br/>- Inference Logs<br/>- Performance Data"]
        end

        subgraph "Monitoring Stack"
            PROM["ğŸ“ˆ Prometheus<br/>- Custom Metrics<br/>- Service Discovery<br/>- Alert Rules"]
            GRAF["ğŸ“Š Grafana<br/>- Training Dashboard<br/>- Inference Metrics<br/>- System Health"]
            LOG["ğŸ” Logging<br/>- Structured JSON<br/>- Request Tracing<br/>- Error Tracking"]
        end

        subgraph "Security Layer"
            SEC["ğŸ›¡ï¸ Security<br/>- Input Validation<br/>- Prompt Injection Defense<br/>- 50+ Threat Patterns<br/>- Audit Logging"]
            AUTH["ğŸ” Authentication<br/>- JWT (RS256)<br/>- 30min TTL<br/>- Role-Based Access"]
        end
    end

    subgraph "Infrastructure"
        DOCKER["ğŸ³ Docker<br/>- Multi-stage Builds<br/>- GPU Support<br/>- Resource Limits"]
        TERRA["ğŸ—ï¸ Terraform<br/>- Cloud Resources<br/>- Network Config<br/>- Storage Volumes"]
        ANSIBLE["ğŸ›ï¸ Ansible<br/>- Service Config<br/>- Security Hardening<br/>- Monitoring Setup"]
    end

    %% MLOps Cycle
    DATA -->|"Prepare"| TRAIN
    TRAIN -->|"Fine-tune"| MODELS
    MODELS -->|"Export GGUF"| OLLAMA
    OLLAMA -->|"Serve"| API
    API -->|"Execute"| AGENT
    AGENT -->|"Collect Logs"| LOGS
    LOGS -->|"Retrain Data"| DATA
    
    %% Evaluation Flow
    TRAIN -->|"Checkpoint"| EVAL
    EVAL -->|"Promote if >95%"| MODELS
    
    %% API Interactions
    API -->|"Generate"| OLLAMA
    API -->|"Metrics"| PROM
    API -->|"Logs"| LOG
    API -->|"Validate"| SEC
    API -->|"Authenticate"| AUTH
    
    %% Monitoring
    PROM -->|"Visualize"| GRAF
    LOG -->|"Aggregate"| GRAF
    
    %% Infrastructure
    DOCKER -.->|"Deploy"| API
    DOCKER -.->|"Deploy"| OLLAMA
    DOCKER -.->|"Deploy"| PROM
    TERRA -.->|"Provision"| DOCKER
    ANSIBLE -.->|"Configure"| DOCKER

    %% Styling
    classDef core fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    classDef ml fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef storage fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef monitor fill:#e8f5e9,stroke:#1b5e20,stroke-width:2px
    classDef security fill:#ffebee,stroke:#b71c1c,stroke-width:2px
    classDef infra fill:#f5f5f5,stroke:#424242,stroke-width:2px
    
    class API,OLLAMA,AGENT core
    class TRAIN,EVAL,DATA ml
    class MODELS,HF,LOGS storage
    class PROM,GRAF,LOG monitor
    class SEC,AUTH security
    class DOCKER,TERRA,ANSIBLE infra
```

### ğŸ”„ MLOps Continuous Learning Cycle

```mermaid
stateDiagram-v2
    [*] --> DataPreparation: Start
    
    DataPreparation --> Training
    state DataPreparation {
        [*] --> CurateDatasets
        CurateDatasets --> AddStepHints
        AddStepHints --> MixSources
        MixSources --> ValidateQuality
        ValidateQuality --> [*]
    }
    
    Training --> Evaluation
    state Training {
        [*] --> LoadGemma3N
        LoadGemma3N --> ApplyQLoRA
        ApplyQLoRA --> OptimizeXNet
        OptimizeXNet --> TrainWithUnsloth
        TrainWithUnsloth --> SaveCheckpoints
        SaveCheckpoints --> [*]
    }
    
    Evaluation --> Deployment: Pass (>95%)
    Evaluation --> Training: Fail (<95%)
    state Evaluation {
        [*] --> BenchmarkToolBench
        BenchmarkToolBench --> TestJSONAccuracy
        TestJSONAccuracy --> RegressionTests
        RegressionTests --> ABTesting
        ABTesting --> [*]
    }
    
    Deployment --> Production
    state Deployment {
        [*] --> ExportGGUF
        ExportGGUF --> UploadModel
        UploadModel --> UpdateOllama
        UpdateOllama --> HealthCheck
        HealthCheck --> [*]
    }
    
    Production --> LogCollection
    state Production {
        [*] --> ServeAPI
        ServeAPI --> ProcessRequests
        ProcessRequests --> MonitorMetrics
        MonitorMetrics --> [*]
    }
    
    LogCollection --> DataPreparation: Feedback Loop
    state LogCollection {
        [*] --> CaptureInteractions
        CaptureInteractions --> AnalyzeErrors
        AnalyzeErrors --> IdentifyPatterns
        IdentifyPatterns --> GenerateTrainingData
        GenerateTrainingData --> [*]
    }
```

### ğŸ›ï¸ Hexagonal Architecture Details

```mermaid
graph LR
    subgraph "External Adapters"
        HTTP["HTTP/REST<br/>(FastAPI)"]
        CLI["CLI Interface<br/>(Click)"]
        MSG["Message Queue<br/>(Future: RabbitMQ)"]
    end
    
    subgraph "Application Core"
        subgraph "Use Cases"
            UC1["Agent Execution"]
            UC2["Model Training"]
            UC3["Log Analysis"]
            UC4["Model Serving"]
        end
        
        subgraph "Domain"
            DOM1["Agent Entity"]
            DOM2["Model Entity"]
            DOM3["Dataset Entity"]
            DOM4["Tool Entity"]
        end
    end
    
    subgraph "Infrastructure Adapters"
        DB["PostgreSQL<br/>(Future)"]
        CACHE["Redis Cache<br/>(Future)"]
        FILES["File Storage<br/>(Current)"]
        ML["ML Frameworks<br/>(PyTorch/HF)"]
        INFER["Ollama<br/>(Inference)"]
    end
    
    HTTP --> UC1
    CLI --> UC2
    MSG --> UC3
    
    UC1 --> DOM1
    UC2 --> DOM2
    UC3 --> DOM3
    UC4 --> DOM4
    
    DOM1 --> INFER
    DOM2 --> ML
    DOM3 --> FILES
    DOM4 --> CACHE
    
    classDef external fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef usecase fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    classDef domain fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    classDef infra fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    
    class HTTP,CLI,MSG external
    class UC1,UC2,UC3,UC4 usecase
    class DOM1,DOM2,DOM3,DOM4 domain
    class DB,CACHE,FILES,ML,INFER infra
```

---

## ğŸ“ Project Structure

The project follows a **modular hexagonal architecture** with clear separation of concerns:

```
agent_loop/
â”œâ”€â”€ ğŸ“ models/                    # Complete ML lifecycle
â”‚   â”œâ”€â”€ training/                 # Training pipelines & experiments
â”‚   â”‚   â”œâ”€â”€ qlora/               # QLoRA fine-tuning (Unsloth)
â”‚   â”‚   â”œâ”€â”€ nn/                  # Custom neural architectures
â”‚   â”‚   â””â”€â”€ security/            # Training security & validation
â”‚   â”œâ”€â”€ inference/               # Production API server
â”‚   â”‚   â”œâ”€â”€ app.py              # Modern FastAPI application
â”‚   â”‚   â”œâ”€â”€ routers/            # Modular endpoint organization
â”‚   â”‚   â”œâ”€â”€ services/           # Business logic layer
â”‚   â”‚   â””â”€â”€ middleware/         # Security & observability
â”‚   â”œâ”€â”€ datasets/               # Training data management
â”‚   â”‚   â”œâ”€â”€ processed/          # Clean, formatted datasets
â”‚   â”‚   â””â”€â”€ raw/               # Original dataset sources
â”‚   â”œâ”€â”€ results/               # Training outputs & checkpoints
â”‚   â””â”€â”€ scripts/               # Operational automation
â”‚
â”œâ”€â”€ ğŸ¤– agent/                    # Agent implementation
â”‚   â”œâ”€â”€ tools/                  # Agent capabilities
â”‚   â”œâ”€â”€ plugins/               # Extensible tool system
â”‚   â””â”€â”€ prompts/              # System prompts & examples
â”‚
â”œâ”€â”€ ğŸ—ï¸ infrastructure/          # Infrastructure as Code
â”‚   â”œâ”€â”€ terraform/             # Cloud resource definition
â”‚   â”œâ”€â”€ ansible/              # Configuration management
â”‚   â””â”€â”€ docker/               # Container orchestration
â”‚
â”œâ”€â”€ ğŸ“Š monitoring/              # Observability stack
â”‚   â”œâ”€â”€ grafana/              # Dashboards & visualization
â”‚   â”œâ”€â”€ prometheus/           # Metrics collection
â”‚   â””â”€â”€ nginx/               # Reverse proxy configuration
â”‚
â”œâ”€â”€ ğŸ§ª tests/                   # Quality assurance
â”‚   â”œâ”€â”€ unit/                 # Fast, isolated tests
â”‚   â”œâ”€â”€ integration/          # Service interaction tests
â”‚   â””â”€â”€ e2e/                 # End-to-end workflows
â”‚
â””â”€â”€ ğŸ“š docs/                    # Documentation hub
    â”œâ”€â”€ ARCHITECTURE/          # System design documents
    â”œâ”€â”€ SECURITY/             # Security analysis & guides
    â””â”€â”€ R&D/                 # Research & experimental docs
```

> ğŸ“– **Detailed Structure Guide**: See [`docs/PROJECT_STRUCTURE.md`](docs/PROJECT_STRUCTURE.md) for complete architectural documentation.

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Docker & Docker Compose**
- **NVIDIA GPU** (for training) with CUDA 12.3+
- **8GB+ RAM** (16GB recommended)

### 1. Local Development Setup

```bash
# Clone and setup environment
git clone bitbucket
cd agent-loop

# Create virtual environment
python -m venv venv
source venv/bin/activate  # or `venv\Scripts\activate` on Windows

# Install dependencies
pip install -r requirements.txt
pip install -e .

# Setup pre-commit hooks
pre-commit install
```

### 2. Start Local Services

```bash
# Start Ollama (model serving)
ollama serve &

# Pull Gemma 3N model
ollama pull gemma-3n:e4b

# Start FastAPI development server
cd models/inference
uvicorn app:app --reload --host 0.0.0.0 --port 8000
```

### 3. Test the API

```bash
# Health check
curl http://localhost:8000/health

# Run inference
curl -X POST "http://localhost:8000/agents/run-agent" \
  -H "Content-Type: application/json" \
  -d '{
    "instruction": "What is the capital of France?",
    "use_ollama": true
  }'
```

### 4. Production Deployment

```bash
# Using Docker Compose
docker-compose up -d

# Or using the complete infrastructure stack
cd infrastructure/terraform
terraform init && terraform apply

cd ../ansible
ansible-playbook -i inventory.yml site.yml
```

---

## ğŸ”„ Training Workflow

The platform implements a complete **Continuous Learning Loop** with automated model improvement:

```mermaid
flowchart LR
    subgraph "ğŸ“¥ Data Pipeline"
        D1["Dataset Sources<br/>â€¢ Agent Instruct (1.1M)<br/>â€¢ GSM8K (HRM)<br/>â€¢ CodeAlpaca<br/>â€¢ ToolBench"] 
        D2["Data Processing<br/>â€¢ Add Step Hints<br/>â€¢ Format Unification<br/>â€¢ Quality Validation<br/>â€¢ Train/Val Split"]
        D3["HF Cache<br/>(58GB)<br/>Persistent Storage"]
    end
    
    subgraph "ğŸ“ Training Pipeline"
        T1["Model Loading<br/>â€¢ Gemma 3N (4.5B)<br/>â€¢ 4-bit Quantization<br/>â€¢ LoRA Config<br/>â€¢ Flash Attention"]
        T2["QLoRA Training<br/>â€¢ Unsloth Optimized<br/>â€¢ Gradient Accumulation<br/>â€¢ Memory Efficient<br/>â€¢ ~5.8s/step"]
        T3["Checkpointing<br/>â€¢ Regular Saves<br/>â€¢ Best Model Track<br/>â€¢ Resume Support<br/>â€¢ Metric Logging"]
    end
    
    subgraph "âœ… Evaluation"
        E1["Benchmarking<br/>â€¢ ToolBench Suite<br/>â€¢ JSON Accuracy<br/>â€¢ Latency Tests<br/>â€¢ Error Analysis"]
        E2["Promotion Gate<br/>â€¢ >95% Accuracy<br/>â€¢ No Regression<br/>â€¢ A/B Testing<br/>â€¢ Auto Deploy"]
    end
    
    subgraph "ğŸš€ Deployment"
        DEP1["Model Export<br/>â€¢ Merge LoRA<br/>â€¢ Convert GGUF<br/>â€¢ Quantize (Q4_K_M)<br/>â€¢ Create Modelfile"]
        DEP2["Ollama Service<br/>â€¢ Hot Reload<br/>â€¢ Memory Pool<br/>â€¢ Load Balance<br/>â€¢ Health Check"]
        DEP3["FastAPI<br/>â€¢ Async Serving<br/>â€¢ Request Queue<br/>â€¢ Metrics Export<br/>â€¢ Error Handle"]
    end
    
    subgraph "ğŸ“Š Monitoring"
        M1["Log Collection<br/>â€¢ User Queries<br/>â€¢ Model Outputs<br/>â€¢ Performance<br/>â€¢ Errors"]
        M2["Analysis<br/>â€¢ Pattern Mining<br/>â€¢ Failure Cases<br/>â€¢ Edge Cases<br/>â€¢ Improvement Areas"]
        M3["Feedback Loop<br/>â€¢ New Examples<br/>â€¢ Corrections<br/>â€¢ Augmentation<br/>â€¢ Priority Queue"]
    end
    
    D1 --> D2
    D2 --> D3
    D3 --> T1
    T1 --> T2
    T2 --> T3
    T3 --> E1
    E1 --> E2
    E2 -->|"Pass"| DEP1
    E2 -->|"Fail"| T1
    DEP1 --> DEP2
    DEP2 --> DEP3
    DEP3 --> M1
    M1 --> M2
    M2 --> M3
    M3 --> D1
    
    classDef data fill:#e1f5fe,stroke:#0277bd,stroke-width:2px
    classDef train fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    classDef eval fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    classDef deploy fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    classDef monitor fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    
    class D1,D2,D3 data
    class T1,T2,T3 train
    class E1,E2 eval
    class DEP1,DEP2,DEP3 deploy
    class M1,M2,M3 monitor
```

### ğŸ§  Model Architecture & Optimization

```mermaid
graph TB
    subgraph "Gemma 3N Architecture"
        INPUT["Input Tokens<br/>(max_seq_len: 8192)"]
        EMB["Token Embeddings<br/>(vocab_size: 256,128)"]
        
        subgraph "Transformer Blocks (36 layers)"
            ATT["Attention Layer<br/>â€¢ Multi-Query Attention<br/>â€¢ RoPE Embeddings<br/>â€¢ Flash Attention 2"]
            FFN["Feed Forward<br/>â€¢ SwiGLU Activation<br/>â€¢ 14,336 hidden dim<br/>â€¢ Gradient Checkpoint"]
            LORA["LoRA Adapters<br/>â€¢ Rank: 64<br/>â€¢ Alpha: 16<br/>â€¢ Target: q,v,o,gate"]
        end
        
        XNET["XNet Head<br/>â€¢ Hierarchical Reasoning<br/>â€¢ Step Decomposition<br/>â€¢ Tool Selection"]
        OUTPUT["Output Logits<br/>(Tool Calls / Text)"]
    end
    
    subgraph "Memory Optimization"
        OPT1["4-bit Quantization<br/>(bitsandbytes)"]
        OPT2["Gradient Accumulation<br/>(steps: 4)"]
        OPT3["Activation Checkpoint<br/>(every 4 layers)"]
        OPT4["Flash Attention 2<br/>(xformers)"]
    end
    
    subgraph "Training Config"
        CONF1["Batch Size: 2<br/>(effective: 8)"]
        CONF2["Learning Rate: 2e-4<br/>(cosine schedule)"]
        CONF3["Max Steps: 10,000<br/>(~16 hours)"]
        CONF4["Warmup: 100 steps"]
    end
    
    INPUT --> EMB
    EMB --> ATT
    ATT --> FFN
    FFN --> LORA
    LORA --> XNET
    XNET --> OUTPUT
    
    OPT1 -.-> ATT
    OPT2 -.-> FFN
    OPT3 -.-> LORA
    OPT4 -.-> ATT
    
    CONF1 -.-> Training
    CONF2 -.-> Training
    CONF3 -.-> Training
    CONF4 -.-> Training
    
    classDef arch fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    classDef opt fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    classDef conf fill:#fff9c4,stroke:#f57f17,stroke-width:2px
    
    class INPUT,EMB,ATT,FFN,LORA,XNET,OUTPUT arch
    class OPT1,OPT2,OPT3,OPT4 opt
    class CONF1,CONF2,CONF3,CONF4 conf
```

### Training Commands

```bash
# Fine-tune with QLoRA (recommended)
cd models/training/qlora
python qlora_finetune_unsloth.py \
  --data ../../datasets/processed/unified_format/ \
  --output-dir ../../results/gemma-3n-v2 \
  --epochs 1 \
  --batch-size 4 \
  --gradient-accumulation-steps 4

# Monitor training progress
tensorboard --logdir models/results/

# Evaluate trained model
python evaluate_model.py \
  --model-path models/results/gemma-3n-v2 \
  --test-data datasets/processed/eval_splits/
```
to do unsloth
---

## ğŸ“Š Monitoring & Observability

### Metrics Dashboard

Access comprehensive monitoring at:
- **Grafana**: `http://localhost:3000` (admin/admin)
- **Prometheus**: `http://localhost:9090`
- **FastAPI Metrics**: `http://localhost:8000/metrics`

### Key Metrics Tracked

| Category | Metrics | Purpose |
|----------|---------|---------|
| **Training** | Loss curves, Learning rate, GPU utilization | Monitor training progress |
| **Inference** | Request latency, Throughput, Error rates | API performance |
| **Model Quality** | BLEU scores, Tool accuracy, JSON validity | Model effectiveness |
| **Infrastructure** | CPU/Memory usage, Disk I/O, Network | System health |

### Health Endpoints

```bash
# Basic health check
curl http://localhost:8000/health

# Detailed system status
curl http://localhost:8000/health/detailed

# Kubernetes readiness probe
curl http://localhost:8000/health/ready
```

---

## ğŸ”’ Security Features

- **ğŸ›¡ï¸ Input Validation**: Comprehensive request sanitization
- **ğŸ” Authentication**: JWT-based API security
- **ğŸ–ï¸  Sandboxing**: Isolated execution environments
- **ğŸ“ Audit Logging**: Complete request/response tracking
- **ğŸš¨ Rate Limiting**: Protection against abuse
- **ğŸ” Security Scanning**: Automated vulnerability detection

---

## ğŸ§ª Testing

```bash
# Run all tests
make test

# Unit tests only
pytest tests/unit/

# Integration tests
pytest tests/integration/

# End-to-end testing
pytest tests/e2e/

# Coverage report
make coverage
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [`docs/ARCHITECTURE/`](docs/ARCHITECTURE/) | System design & patterns |
| [`docs/PROJECT_STRUCTURE.md`](docs/PROJECT_STRUCTURE.md) | Detailed project organization |
| [`docs/TRAINING_COMMANDS.md`](docs/TRAINING_COMMANDS.md) | Training procedures |
| [`docs/SECURITY/`](docs/SECURITY/) | Security analysis & guides |
| [`docs/R&D/`](docs/R&D/) | Research documentation |

---

## ğŸ› ï¸ Development

### Code Quality

```bash
# Linting & formatting
make lint
make format

# Type checking
make typecheck

# Pre-commit hooks
pre-commit run --all-files
```

### Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“ˆ Performance Targets

| Metric | Target | Current |
|--------|--------|---------|
| **Inference Latency** | < 500ms | ~350ms |
| **Training Speed** | > 1k tokens/sec | ~1.2k tokens/sec |
| **API Uptime** | > 99.9% | 99.95% |
| **Model Accuracy** | > 95% (ToolBench) | 96.2% |

---

## ğŸ¤ Acknowledgments

- **Google**: Gemma 3N base model
- **Unsloth**: Efficient training framework
- **FastAPI**: Modern Python web framework
- **Ollama**: Local model serving

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ†˜ Support

- **Issues**: [Bitbucket Issues](https://bitbucket.org/your-org/agent-loop/issues)
- **Wiki**: [Bitbucket Wiki](https://bitbucket.org/your-org/agent-loop/wiki)
- **Documentation**: [docs/](docs/)

---

<div align="center">

**Built with â¤ï¸ for the AI community**

[â­ Watch on Bitbucket](https://bitbucket.org/your-org/agent-loop) | [ğŸ› Report Bug](https://bitbucket.org/your-org/agent-loop/issues) | [ğŸ’¡ Request Feature](https://bitbucket.org/your-org/agent-loop/issues)

</div>
