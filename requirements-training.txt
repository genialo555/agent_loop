# ML Training Dependencies for Fine-tuning Sprint 2
# Based on 2024 best practices: PyTorch 2.7, CUDA 12.6, Transformers 4.46

# Core ML Framework - DK002: Pin exact versions for reproducibility
torch==2.7.1
torchvision==0.22.1  
torchaudio==2.7.1
--extra-index-url https://download.pytorch.org/whl/cu126

# Transformers and Fine-tuning Stack
transformers==4.46.2
accelerate==1.1.1
peft==0.13.2
trl==0.12.1

# Quantization and Memory Optimization
bitsandbytes==0.45.2

# Data Processing and Utilities
datasets==3.1.0
tokenizers==0.20.3
sentencepiece==0.2.0

# Training Utilities
wandb>=0.18.0                    # Experiment tracking
tensorboard>=2.18.0             # Local training monitoring
evaluate>=0.4.0                 # Model evaluation metrics
scikit-learn>=1.3.0            # Additional ML utilities

# Memory and Performance Optimization
deepspeed>=0.15.0               # Distributed training (optional)
flash-attn>=2.6.0              # Flash attention optimization
xformers>=0.0.28               # Memory efficient transformers

# Development and Debugging
ipykernel>=6.29.0              # Jupyter kernel for interactive development
matplotlib>=3.8.0              # Plotting training curves
seaborn>=0.13.0                # Statistical visualization

# Security and Validation
safetensors>=0.4.0             # Safe tensor serialization
huggingface-hub>=0.26.0        # Model hub integration with security

# Additional Dependencies for QLoRA and Custom Heads
scipy>=1.11.0                  # Mathematical functions
numpy>=1.24.0                 # Numerical computing
pandas>=2.1.0                 # Data manipulation

# Note: Keep base torch>=2.0.0 in main requirements.txt for inference compatibility
# This file contains training-specific heavy dependencies that should only be
# installed in training containers to maintain lean production images